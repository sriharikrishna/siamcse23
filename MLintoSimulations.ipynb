{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Machine Learning into Simulations\n",
        "\n",
        "## About\n",
        "\n",
        "While the connection between simulations and machine learning enabled by automatic differentiation seeks to embed simulation, with added gradient information, into machine learning frameworks, there also exists the other way - embedding (trained) machine learning models into simulations. In this notebook we will go through the different approaches available to us to embed machine learning models into our simulations, breaking it down by programming language (C, C++, Fortran). Especially for this task there is a wide variety of tools available to us with their own constraints, and design philosophies."
      ],
      "metadata": {
        "id": "IVerAjStimV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "* [1. Integration into C](#export-into-c)\n",
        "  * [1.1 Export with TensorFlow Lite](#tf-lite)\n",
        "  * [1.2 Export with TVM's C Runtime](#_tvm)\n",
        "  * [1.3 Export with ONNX](#_onnx)\n",
        "  * [1.4 Export with IREE](#_iree)\n",
        "* [2. Integration into C++](#integrate-cpp)\n",
        "  * [2.1 Export with TorchScript](#torchscript)\n",
        "  * [2.2 Export with TVM](#tvm-cpp)\n",
        "  * [2.3 Export with ONNX](#onnx-cpp)\n",
        "  * [2.4 Export with IREE](#iree-cpp)\n",
        "  * [2.5 Export with TensorFlow Lite](#tflite-cpp)\n",
        "  * [2.6 Export with AOT-compiled XLA](#xla-cpp)\n",
        "* [3. Integration into Fortran](#export-into-fortran)\n",
        "  * [3.1 Export with IREE](#iree-fort)\n",
        "    * [3.1.1 Emit C Code](#iree-fort-c)\n",
        "    * [3.1.2 Compile a Static Library](#iree-fort-static)\n",
        "\n",
        "  > The options for export into Fortran are largely the ones of C, as such we only present one example here."
      ],
      "metadata": {
        "id": "ThPHZcHasb5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Integration into C <a name=\"export-into-c\"></a>"
      ],
      "metadata": {
        "id": "FwIZ1wGMimTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the integration of a trained machine learning model into a C based simulation, we begin with the example of PyTorch and show how we can export a model exploring the model export process from a trained JAX model to a TensorFlow model, then using TensorFlow Lite to export the model to C.\n",
        "\n",
        "![](https://i.imgur.com/YZ0xFz0.png)\n",
        "\n",
        "For the sake of this tutorial, we will henceforth consider the following pretrained ResNet-50 PyTorch model to include in our simulation:\n",
        "\n",
        "```python\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Initializing the model with the pre-trained weights, and setting it into evaluation mode\n",
        "pretrained_weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=pretrained_weights)\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "With this we can now consider how to include said model in our simulations.\n",
        "\n",
        "> An aspect we are (consciously) glossing over is the data structure exchange between different frameworks. For this we point the reader to the [DLPack](https://dmlc.github.io/dlpack/latest/) documentation and Python's [Array API specification](https://data-apis.org/array-api/2022.12/)."
      ],
      "metadata": {
        "id": "UM3CTjOcuWh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Export with [TensorFlow Lite](https://www.tensorflow.org/lite) <a name=\"tf-lite\"></a>"
      ],
      "metadata": {
        "id": "qFSXP9HfuYOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To export a trained machine learning model into a simulation framework from JAX, or TensorFlow you have multiple options. Coming from JAX most of these begins with converting the JAX model into a TensorFlow model, the exception being IREE where one can export straight from JAX using the [IREE Runtime](#_iree).\n",
        "4\n",
        "1. Write JAX model & train it\n",
        "2. Export to TensorFlow with `jax2tf` [link](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py)\n",
        "3. Utilize TensorFlow's infrastructure to include your model\n",
        "\n",
        "While this inclusion of the model can take the form of the presented cross-framework infrastructure afforded by TVM, ONNX, and IREE the JAX/TensorFlow ecosystem also has its own [model export to C](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_objective-c) infrastructure in the form of [TensorFlow Lite](https://www.tensorflow.org/lite).\n",
        "\n",
        "In the interest of focussing on PyTorch as our main machine learning framework for this tutorial, we are omitting a TensorFlow Lite example. If you are interested in exporting TensorFlow models into your C based simulation we would encourage you to take a look at the following selected TensorFlow Lite examples, which serve as good illustrations for the workflow and required project structure:\n",
        "\n",
        "* [Image Classifier in C](https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/vision/sources/TFLImageClassifier.h)"
      ],
      "metadata": {
        "id": "GQ_pHRopuYLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Export with [TVM](https://tvm.apache.org) <a name=\"_tvm\"></a>\n"
      ],
      "metadata": {
        "id": "75pFncw1uYI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TVM is able to utilize the graph representation emitted by PyTorch's JIT, or other frameworks in large parts, which it ingests and then transforms into its own internal graph representation called `Relay`. To then leverage that graph representation to let the TVM runtime library run on any device we need to\n",
        "\n",
        "* [Build the TVM runtime library](https://tvm.apache.org/docs/how_to/deploy/index.html#build-the-tvm-runtime-library)\n",
        "\n",
        "After which we can integrate the runtime API into our build system, and integrate the machine learning model exported with TVM in our simulation.\n",
        "\n",
        "* [Integrate TVM into Your Project](https://tvm.apache.org/docs/how_to/deploy/integrate.html)\n",
        "\n",
        "For which we want to use the [C_Runtime_API](https://github.com/apache/tvm/blob/main/src/runtime/c_runtime_api.cc#L262) to then integrate the machine learning model on the C level."
      ],
      "metadata": {
        "id": "RI6opHwBuYGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Export with [ONNX](https://onnxruntime.ai) <a name=\"_onnx\"></a>"
      ],
      "metadata": {
        "id": "6EAJr6qluYDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONNX (Open Neural Network Exchange) is one of the main standards to exchange models between frameworks and one of the main promoters of interoperability between frameworks. To work with ONNX we first need to export the model from PyTorch into a ONNX module\n",
        "\n",
        "```python\n",
        "# Export the model\n",
        "torch.onnx.export(resnet50,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"resnet50.onnx\",           # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})\n",
        "```\n",
        "\n",
        "after which we want to utilize the [ONNX Runtime](https://onnxruntime.ai) to integrate the machine learning model into our simulations. Afterwards we will follow a similar approach to other frameworks to include an ONNX model in our simulation:\n",
        "\n",
        "1. Include `onnxruntime_c_api.h`\n",
        "2. Call `OrtCreateEnv`\n",
        "3. Create a session: `OrtCreateSession`\n",
        "4. Create a tensor `OrtCreateMemoryInfo` & `OrtCreateTensorWithDataAsOrtValue`\n",
        "5. `OrtRun`\n",
        "\n",
        "A good example to see at play is the [FNS Candy](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/fns_candy_style_transfer) from the ONNX runtime example directory and the [ONNX Runtime C API Documentation](https://onnxruntime.ai/docs/get-started/with-c.html)."
      ],
      "metadata": {
        "id": "gYMFGKzXuYA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Export with [IREE](https://iree-org.github.io/iree/) <a name=\"_iree\"></a>\n",
        "\n",
        "To fully leverage IREE's C API we have to begin by building its runtime from source to then utilize its components inside of our simulation.\n",
        "\n",
        "* [Building from Source - Getting Started](https://openxla.github.io/iree/building-from-source/getting-started/)\n",
        "\n",
        "IREE is then able to use its own minimal virtual machine at runtime, with more complex logic being dispatched to the chosen hardware backend through a hardware abstraction layer (HAL) interface. As simulation users, seeking to embed our machine learning models into simulations we will mostly be interacting with either the virtual machine or the hardware abstraction layer.\n",
        "\n",
        "> We lower further down to C if we'd seek to embed native C-code into our simulation, and avoid the IREE runtime dependency.\n",
        "\n",
        "The involved steps to take a pretrained PyTorch model, and integrate it into our simulation code are the following:\n",
        "\n",
        "1. Compilation with Torch-MLIR\n",
        "2. Compilation to IREE's vmfb format\n",
        "3. Register the components with IREE\n",
        "4. Create an instance of the Virtual Machine\n",
        "5. Load the pretrained model\n",
        "6. Create the VM context"
      ],
      "metadata": {
        "id": "oHUsbrMguX7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compile the pre-trained model with [Torch-MLIR](https://github.com/llvm/torch-mlir) into the linear algebra dialect of [MLIR](https://mlir.llvm.org) as a first step."
      ],
      "metadata": {
        "id": "Cfpi58divDPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "inference_args = (pretrained_weights, X_test)\n",
        "graph = functorch.make_fx(model)(*inference_args)\n",
        "strip_overloads(graph)\n",
        "linalg_on_tensors_mlir = torch_mlir.compile(\n",
        "    graph,\n",
        "    inference_args,\n",
        "    output_type=\"linalg-on-tensors\")\n",
        "```"
      ],
      "metadata": {
        "id": "BrEMd5jFvDM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compile the generated linear algebra dialect into IREEs own `vmfb` standard to then be usable by the C API.\n",
        "\n",
        "> We can accumulate a library of precompiled machine learning models which we can then include in our simulations that way. "
      ],
      "metadata": {
        "id": "w1Q4ijOPvDKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "iree_torch.compile_to_vmfb(\n",
        "        linalg_on_tensors_mlir, args.iree_backend)\n",
        "```"
      ],
      "metadata": {
        "id": "gkjV2_fnvDG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c\n",
        "// Including the headers\n",
        "#include \"iree/base/api.h\"\n",
        "#include \"iree/hal/api.h\"\n",
        "#include \"iree/vm/api.h\"\n",
        "\n",
        "// Include VM bytecode and HAL modules + HAL drivers in use\n",
        "#include \"iree/modules/hal/module.h\"\n",
        "#include \"iree/hal/drivers/local_task/registration/driver_module.h\"\n",
        "#include \"iree/vm/bytecode_module.h\"\n",
        "```"
      ],
      "metadata": {
        "id": "8z-LfCjVvDD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Register the individual components\n",
        "\n",
        "```c\n",
        "iree_hal_local_task_driver_module_register(\n",
        "    iree_hal_driver_registry_default());\n",
        "```"
      ],
      "metadata": {
        "id": "Q3bsohUgvDAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a VM instance\n",
        "\n",
        "```c\n",
        "iree_vm_instance_t* instance = NULL;\n",
        "iree_vm_instance_create(iree_allocator_system(), &instance);\n",
        "\n",
        "// Modules with custom types must be statically registered before use.\n",
        "iree_hal_module_register_all_types(instance);\n",
        "\n",
        "// Using the CPU driver\n",
        "iree_hal_driver_t* driver = NULL;\n",
        "iree_hal_driver_registry_try_create(...);\n",
        "\n",
        "// Using the default device here\n",
        "iree_hal_device_t* device = NULL;\n",
        "iree_hal_driver_create_default_device(...);\n",
        "\n",
        "// Create a HAL module for the newly created VM\n",
        "iree_vm_module_t* hal_module = NULL;\n",
        "iree_hal_module_create(...);\n",
        "\n",
        "// Releasing reference to the driver\n",
        "iree_hal_driver_release(driver);\n",
        "```\n",
        "\n",
        "5. Load the pretrained PyTorch model as a `.vmfb` bytecode module, which we generated above\n",
        "\n",
        "```c\n",
        "iree_vm_module_t* bytecode_module = NULL;\n",
        "iree_vm_bytecode_module_create(\n",
        "    instance,\n",
        "    iree_const_byte_span_t{module_data, module_size},\n",
        "    /*flatbuffer_allocator=*/iree_allocator_null(),\n",
        "    /*allocator=*/iree_allocator_system(), &bytecode_module);\n",
        "```\n",
        "\n",
        "6. Create the VM context\n",
        "\n",
        "```c\n",
        "iree_vm_context_t* context = NULL;\n",
        "iree_vm_module_t* modules[2] = {hal_module, bytecode_module};\n",
        "iree_vm_context_create_with_modules(\n",
        "    instance, IREE_VM_CONTEXT_FLAG_NONE,\n",
        "    IREE_ARRAYSIZE(modules), modules,\n",
        "    iree_allocator_system(), &context);\n",
        "\n",
        "// Release references to the modules\n",
        "iree_vm_module_release(hal_module);\n",
        "iree_vm_module_release(bytecode_module);\n",
        "```\n",
        "\n",
        "Looking up the function call\n",
        "\n",
        "```c\n",
        "iree_vm_function_t main_function;\n",
        "iree_vm_context_resolve_function(\n",
        "    context, iree_string_view_literal(\"module.main_function\"), &main_function)\n",
        ");\n",
        "```\n",
        "\n",
        "After which we can finally invoke our pretrained machine learning model from within our simulation model\n",
        "\n",
        "```c\n",
        "// (Application-specific I/O buffer setup, making data available to the device)\n",
        "iree_vm_invoke(context, main_function, IREE_VM_INVOCATION_FLAG_NONE,\n",
        "               /*policy=*/NULL, inputs, outputs,\n",
        "               iree_allocator_system());\n",
        "```\n",
        "\n",
        "> We can decorate every call here with `IREE_CHECK_OK` to catch (silent) errors while we are debugging the workflow."
      ],
      "metadata": {
        "id": "6c7UGyztvC9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an end-to-end example in C with CMake, please take a look at the following MNIST example for IREE:\n",
        "\n",
        "* [IREE Samples: Vision Inference Example](https://github.com/iree-org/iree-samples/tree/main/cpp/vision_inference)\n",
        "\n",
        "For a minimal example of what an integration of an IREE layer in a simulation code would look like with inputs, and outputs from the VM we recommend the simple embedding example:\n",
        "\n",
        "* [IREE Samples: Simple Embedding](https://github.com/openxla/iree/tree/main/samples/simple_embedding)\n",
        "\n",
        "With the native lowering to C poorly documented at the current time, we recommend taking a look at the [tests](https://github.com/openxla/iree/tree/89b2201f717e16204f550e72cdd54a8901e77fdc/iree/compiler/Dialect/VM/Target/C/test) of the C-target, which point us to having to invoke iree through the command line interface to then emit a C-module. The syntax for this is\n",
        "\n",
        "```bash\n",
        "iree-translate -iree-vm-ir-to-c-module ResNet50.vmfb\n",
        "```\n",
        "\n",
        "where we built the `iree-translate` binary when we compiled IREE from source."
      ],
      "metadata": {
        "id": "njT4Yh80vC65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Integration into C++ <a name=\"integrate-cpp\"></a>\n",
        "\n",
        "For the integration of a trained machine learning model into a C++ based simulation, we begin with the example of PyTorch and show how we can export a model using TorchScript, TVM, and ONNX before looking at the tools available in the Tensorflow/JAX ecosystem.\n",
        "\n",
        "![](https://i.imgur.com/e7mGvir.png)\n",
        "\n",
        "For the sake of this tutorial, we will henceforth consider the following pretrained ResNet-50 PyTorch model to include in our simulation:\n",
        "\n",
        "```python\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Initializing the model with the pre-trained weights, and setting it into evaluation mode\n",
        "pretrained_weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=pretrained_weights)\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "With this we can now consider how to include said model in our simulations.\n",
        "\n",
        "> An aspect we are (consciously) glossing over is the data structure exchange between different frameworks. For this we point the reader to the [DLPack](https://dmlc.github.io/dlpack/latest/) documentation and Python's [Array API specification](https://data-apis.org/array-api/2022.12/)."
      ],
      "metadata": {
        "id": "fFao1x4YimRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Export with [TorchScript](https://pytorch.org/docs/stable/jit.html)  <a name=\"torchscript\"></a>"
      ],
      "metadata": {
        "id": "ow34CDnL0ld9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To now apply PyTorch's own utility for the export of PyTorch's models to C++ [TorchScript]() we have to begin by tracing our machine learning model\n",
        "\n",
        "```python\n",
        "traced_model = torch.jit.trace(model, (**example_inputs))\n",
        "```\n",
        "\n",
        "this has now created a `torch.jit.ScriptModule` of which `TracedModule` is an instance. This records the definitions into an intermediate representation/graph. This approach has many advantages:\n",
        "\n",
        "* TorchScript can be invoked by itw own interpreter\n",
        "* We can save the model, and reload it from the storage place later on\n",
        "* Interfacing to different backends/devices\n",
        "\n",
        "> Attention: TorchScript tracing only records the code-path **taken** not the entire code!!!\n",
        "\n",
        "This restriction of TorchScript can be circumvented by using a _script compiler_ this would be done with\n",
        "\n",
        "```python\n",
        "scripted_control = torch.jit.script(ControlFlowCode())\n",
        "\n",
        "resnet = ResNet50(scripted_control)\n",
        "scripted_resnet = torch.jit.script(resnet)\n",
        "```\n",
        "\n",
        "The files we generate with this all have the `.pt` format and can be stored or loaded with\n",
        "\n",
        "```python\n",
        "traced.save('scripted_resnet.pt')\n",
        "loaded = torch.jit.load('scripted_resnet.pt')\n",
        "```\n",
        "\n",
        "which we can then [load into C++](https://pytorch.org/tutorials/advanced/cpp_export.html)."
      ],
      "metadata": {
        "id": "nyn6pjQd0la_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load the model we then add the following syntax to our C++ code\n",
        "\n",
        "```cpp\n",
        "#include <torch/script.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <memory>\n",
        "\n",
        "int main(int argc, const char* argv[]) {\n",
        "\n",
        "  torch::jit::script::Module module;\n",
        "  try {\n",
        "    // Deserialize the ScriptModule from a file using torch::jit::load().\n",
        "    module = torch::jit::load(argv[1]);\n",
        "  }\n",
        "  catch (const c10::Error& e) {\n",
        "    std::cerr << \"error loading the model\\n\";\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  std::cout << \"Model loaded\\n\";\n",
        "}\n",
        "```\n",
        "\n",
        "where the `</torch/script.h>` encompasses all dependencies on the LibTorch library to run the example, and then deserializes the module using `torch::jit::load()` taking the file path as an input, and then receive a `torch::jit::script::Module` object in return. The required added syntax for CMake is then\n",
        "\n",
        "```cmake\n",
        "find_package(Torch REQUIRED)\n",
        "\n",
        "add_executable(resnet-50 resnet-50.cpp)\n",
        "target_link_libraries(resnet-50 \"${TORCH_LIBRARIES}\")\n",
        "set_property(TARGET resnet-50 PROPERTY CXX_STANDARD 14)\n",
        "```\n",
        "\n",
        "the only thing missing to run this with CMake is then having [LibTorch](https://pytorch.org/cppdocs/installing.html) present on your device, which you are able to [download](https://pytorch.org/get-started/locally/) from the PyTorch release page. And then build\n",
        "\n",
        "```bash\n",
        "mkdir build && cd build\n",
        "cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n",
        "cmake --build . --config Release\n",
        "```\n",
        "\n",
        "With which we can run our minimal example."
      ],
      "metadata": {
        "id": "dfBeNidb0lX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further exploration of the TorchScript pathway there are multiple resources to consult:\n",
        "\n",
        "* [Examples Folder](https://github.com/pytorch/pytorch/tree/master/test/custom_operator)\n",
        "* [TorchScript Reference Documentation](https://pytorch.org/docs/master/jit.html)\n",
        "* [PyTorch C++ API Documentation](https://pytorch.org/cppdocs/)\n",
        "* [PyTorch Docs](https://pytorch.org/docs/)"
      ],
      "metadata": {
        "id": "N-COVBTH0lT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Export with [TVM](https://tvm.apache.org)  <a name=\"tvm-cpp\"></a>"
      ],
      "metadata": {
        "id": "wZNGhFJ50lOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TVM is able to utilize the graph representation emitted by PyTorch's JIT in large parts, which it ingests and then transforms into its own internal graph representation called `Relay`. Taking over the first few lines of the TorchScript example\n",
        "\n",
        "```python\n",
        "scripted_control = torch.jit.script(ControlFlowCode())\n",
        "\n",
        "resnet = ResNet50(scripted_control)\n",
        "scripted_resnet = torch.jit.script(resnet)\n",
        "```\n",
        "\n",
        "Which we can then convert into a Relay graph\n",
        "\n",
        "```python\n",
        "input_name = \"input0\"\n",
        "shape_list = [(input_name, img.shape)]\n",
        "mod, params = relay.frontend.from_pytorch(scripted_resnet, shape_list)\n",
        "```\n",
        "\n",
        "With this representation, we now live inside of the TVM compiler and are subsequently able to compile the model to any backend TVM supports, such as on CPU machine for this example. For this one first compiles with LLVM before the utilizing a `graph_executor` to deploy the graph\n",
        "\n",
        "```python\n",
        "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
        "dev = tvm.cpu(0)\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target=target, params=params)\n",
        "```\n",
        "\n",
        "Which is then deployed as a graph\n",
        "\n",
        "```python\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dtype = \"float32\"\n",
        "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "m.set_input(input_name, tvm.nd.array(img.astype(dtype)))\n",
        "m.run()\n",
        "tvm_output = m.get_output(0)\n",
        "```\n",
        "\n",
        "The export into C++ works similarly, and can best be inspected in\n",
        "\n",
        "* [Deploy a TVM Module using the C++ API](https://tvm.apache.org/docs/how_to/deploy/cpp_deploy.html)\n",
        "* [The Deployment Guide for C++](https://github.com/apache/tvm/tree/main/apps/howto_deploy)"
      ],
      "metadata": {
        "id": "MH8E14EC7Glf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Export with [ONNX](https://onnxruntime.ai/docs/reference/ort-format-models.html)  <a name=\"onnx-cpp\"></a>"
      ],
      "metadata": {
        "id": "euCcXnZO0lJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONNX (Open Neural Network Exchange) is one of the main standards to exchange models between frameworks and one of the main promoters of interoperability between frameworks. To work with ONNX we first need to export the model from PyTorch into a ONNX module\n",
        "\n",
        "```python\n",
        "# Export the model\n",
        "torch.onnx.export(resnet50,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"resnet50.onnx\",           # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})\n",
        "```\n",
        "\n",
        "after which we want to utilize the [ONNX Runtime](https://onnxruntime.ai) to integrate the machine learning model into our simulations. Afterwards we will follow a similar approach to other frameworks to include an ONNX model in our simulation:\n",
        "\n",
        "1. Include `onnxruntime_cxx_api.h`\n",
        "2. Call `OrtCreateEnv`\n",
        "3. Create a session: `OrtCreateSession`\n",
        "4. Create a tensor `OrtCreateMemoryInfo` & `OrtCreateTensorWithDataAsOrtValue`\n",
        "5. `OrtRun`\n",
        "\n",
        "A good example to see at play is the [FNS Candy](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/fns_candy_style_transfer) from the ONNX runtime example directory."
      ],
      "metadata": {
        "id": "d1aZZnrd2WgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Export with [IREE](https://iree-org.github.io/iree/) <a name=\"iree-cpp\"></a>"
      ],
      "metadata": {
        "id": "SFITJ-O92Wdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google's IREE is another framework, which is able to ingest any model coming from PyTorch, JAX, and TensorFlow, export, and then expose it to a general programming language. IREE's C-API is the main export for the purpose of this tutorial, but there exists a semi-official template for C++ maintained by the _Fraunhofer Institute for AI and Autonomous Systems_ which shows IREE can be integrated with any C++ based project. We will go into more detail on IREE, and its syntax for model export in the tutorial on C, and would hence urge a look at the IREE subsection in the C-Tutorial if you are interested in utilizing IREE in your simulation framework, or a look at the IREE section of the slides if you are looking to scope its capabilities.\n",
        "\n",
        "* [IREE Template for C++](https://github.com/iml130/iree-template-cpp)"
      ],
      "metadata": {
        "id": "86Uh99Os2WbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Export with [TensorFlow Lite (TFLite)](https://www.tensorflow.org/lite/microcontrollers/build_convert)  <a name=\"tflite-cpp\"></a>"
      ],
      "metadata": {
        "id": "fs9xVHkV2WYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To export a trained machine learning model into a simulation framework from JAX, or TensorFlow you have multiple options. Coming from JAX most of these begins with converting the JAX model into a TensorFlow model\n",
        "\n",
        "1. Write JAX model & train it\n",
        "2. Export to TensorFlow with `jax2tf` [link](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py)\n",
        "3. Utilize TensorFlow's infrastructure to include your model\n",
        "\n",
        "While this inclusion of the model can take the form of the above presented cross-framework infrastructure afforded by TVM, ONNX, and IREE the JAX/TensorFlow ecosystem also has its own [model export to C++](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c) infrastructure in the form of [TensorFlow Lite](https://www.tensorflow.org/lite).\n",
        "\n",
        "In the interest  on focussing on PyTorch as our main machine learning framework for this tutorial, we are omitting a TensorFlow Lite example. If you are interested in exporting TensorFlow models into your C++ based simulation we would encourage you to take a look at the following selected TensorFlow Lite examples, which serve as good illustrations for the workflow and required project structure:\n",
        "\n",
        "* [Minimal Example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc)\n",
        "* [Image Labeling](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc)"
      ],
      "metadata": {
        "id": "pIVJoZGO2WWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Export with Ahead-of-Time (AOT) Compilation with [XLA](https://www.tensorflow.org/xla)  <a name=\"xla-cpp\"></a>"
      ],
      "metadata": {
        "id": "gJHfeyn32WT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow2, through its compilation backend, equips the user with the powerful ability to ahead-of-time (AOT) compile the XLA-graph generated by the machine learning model into a C++ model. For this we broadly follow the following steps:\n",
        "\n",
        "1. Train the TensorFlow model\n",
        "2. Save the model with `tf.saved_model.save`\n",
        "3. Perform AOT compilation with XLA\n",
        "4. Create a shared library\n",
        "5. Create the C++ file\n",
        "6. Build with your favorite build system"
      ],
      "metadata": {
        "id": "JHp0qcXd2WRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presuming that we have our ResNet-50 model trained in TensorFlow, we then used the [saved model](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) format, and store it\n",
        "\n",
        "```python\n",
        "class ResNet50(tf.Module):\n",
        "    ...\n",
        "\n",
        "model = ResNet50()\n",
        "tf.saved_model.save(\n",
        "    model, './resnet', signature=None, options=None\n",
        ")\n",
        "```\n",
        "\n",
        "Which then gives us a model which can take in an input tensor, and then run the model in a frozen (inference) state.\n",
        "\n",
        "> Minimal example of the saved model API available [here](https://www.tensorflow.org/guide/saved_model)\n",
        "\n",
        "From there we then use the `saved_model_cli` from the command line to perform AOT compilation with XLA\n",
        "\n",
        "```bash\n",
        "saved_model_cli aot_compile_cpu \\\n",
        "    --dir ./resnet/1 \\\n",
        "    --tag_set serve \\\n",
        "    --signature_def_key serving_default \\\n",
        "    --output_prefix compiled_resnet \\\n",
        "    --cpp_class Bar\n",
        "```\n",
        "\n",
        "We then need to build the XLA AOT Runtime for that we need to\n",
        "\n",
        "```bash\n",
        "cd /path/to/TensorFlow/xla_aot_runtime_src\n",
        "cmake .\n",
        "make\n",
        "```\n",
        "\n",
        "With which we then have the respective runtime, and have to invoke the subgraph from our simulation code just as done in this [subgraph invocation example](https://www.tensorflow.org/xla/tfcompile#step_3_write_code_to_invoke_the_subgraph), which gives us a `resnet50_invocation.cc`. Compiling it then with\n",
        "\n",
        "```bash\n",
        "clang++ resnet50_invocation.cc -I/resnet50.o -L/libtf_xla_runtime.a\n",
        "```\n",
        "\n",
        "which can be integrated into the build system of our simulation such as Makefile, or CMake."
      ],
      "metadata": {
        "id": "-E_iNIaR2WNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Integration into Fortran <a name=\"export-into-fortran\"></a>\n"
      ],
      "metadata": {
        "id": "IGJgjvLGimO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the integration of a trained machine learning model into a Fortran based simulation, we note that the main export paths are the ones presented in the export-section of C:\n",
        "\n",
        "![](https://i.imgur.com/qLb7qEX.png)\n",
        "\n",
        "> Am missing the IREE path in this figure\n",
        "\n",
        "For the sake of this tutorial, we will henceforth consider the following pretrained ResNet-50 PyTorch model to include in our simulation:\n",
        "\n",
        "```python\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Initializing the model with the pre-trained weights, and setting it into evaluation mode\n",
        "pretrained_weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=pretrained_weights)\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "With Fortran mostly relying on the provided C-APIs, we will sketch one end-to-end example of what such a model export would look like for the above pretrained PyTorch model to Fortran.\n",
        "\n",
        "> An aspect we are (consciously) glossing over is the data structure exchange between different frameworks. For this we point the reader to the [DLPack](https://dmlc.github.io/dlpack/latest/) documentation and Python's [Array API specification](https://data-apis.org/array-api/2022.12/)."
      ],
      "metadata": {
        "id": "Pe5Nn5hGimMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Exporting a Trained PyTorch Model to Fortran with IREE <a name=\"iree-fort\"></a>"
      ],
      "metadata": {
        "id": "qeaZbCN5imKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To integrate a trained PyTorch model into Fortran we have two main options, which broadly follow the approaches taken for C while choosing to cover the two main approaches with IREE here as the example for Fortran. For this we need to have a source install of IREE on the system we perform the below model conversion on\n",
        "\n",
        "* [Building from Source - Getting Started](https://openxla.github.io/iree/building-from-source/getting-started/)\n",
        "\n",
        "After which we have two main pathways available to us, the two of them being:\n",
        "\n",
        "1. Let IREE emit C code and treat it as normal C code.\n",
        "2. Compile the model into a static library with IREE, and then link the static library into our code.\n",
        "\n",
        "Both of which are highly viable paths and build on the Fortran C interface, which most large simulation codes already use today for e.g. system calls and can hence easily be integrated into existing build systems."
      ],
      "metadata": {
        "id": "oznJUBsgimHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1 Emit C Code <a name=\"iree-fort-c\"></a>"
      ],
      "metadata": {
        "id": "NrMTOArN3Grj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as shown in the section on C, we have to begin at the Python level to get from the trained representation PyTorch has of our model into a `vmfb` module that IREE can then take to convert to native C code. Beginning by converting the PyTorch code with [Torch-MLIR](https://github.com/llvm/torch-mlir) into a [MLIR](https://mlir.llvm.org) [Linalg](https://mlir.llvm.org/docs/Dialects/Linalg/) module\n",
        "\n",
        "```python\n",
        "inference_args = (pretrained_weights, X_test)\n",
        "graph = functorch.make_fx(model)(*inference_args)\n",
        "strip_overloads(graph)\n",
        "linalg_on_tensors_mlir = torch_mlir.compile(\n",
        "    graph,\n",
        "    inference_args,\n",
        "    output_type=\"linalg-on-tensors\")\n",
        "```\n",
        "\n",
        "After which we can use `iree-torch` to build this into a `vmfb` module for IREE.\n",
        "\n",
        "```python\n",
        "iree_torch.compile_to_vmfb(linalg_on_tensors_mlir, args.iree_backend)\n",
        "```\n",
        "\n",
        "with which we have our `vmfb` module, and can then convert this with the command line interface into native C code.\n",
        "\n",
        "> There also exist [iree-jax](https://openxla.github.io/iree/getting-started/jax/), and [iree-tensorflow](https://openxla.github.io/iree/getting-started/tensorflow/) to follow this path from the JAX, and TensorFlow ecosystem."
      ],
      "metadata": {
        "id": "FG5pZ_bx3Go8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the command line we then have to use the `iree-translate` binary we previously built to convert from a `vmfb` representation to native C code\n",
        "\n",
        "```bash\n",
        "iree-translate -iree-vm-ir-to-c-module ResNet50.vmfb\n",
        "```\n",
        "\n",
        "which we can then readily integrate just as any other Code into our build system."
      ],
      "metadata": {
        "id": "ZF8wJSHM3Gms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.2 Compile a Static Library <a name=\"iree-fort-static\"></a>"
      ],
      "metadata": {
        "id": "nR47wapJ3GkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To take the route of compiling a static library, we will first have to obtain the MLIR representation, produce the static library, and then explore the usage of the provided function in a hypothetical simulation. For this we will again utilize Torch-MLIR\n",
        "\n",
        "```python\n",
        "inference_args = (pretrained_weights, X_test)\n",
        "graph = functorch.make_fx(model)(*inference_args)\n",
        "strip_overloads(graph)\n",
        "linalg_on_tensors_mlir = torch_mlir.compile(\n",
        "    graph,\n",
        "    inference_args,\n",
        "    output_type=\"linalg-on-tensors\")\n",
        "```\n",
        "\n",
        "with which we then have an MLIR module of our trained model in `resnet_50.mlir`. Compiling this now with IREE we will obtain three output files\n",
        "\n",
        "* `resnet_50.h`\n",
        "* `resnet_50.o`\n",
        "* `resnet_50.vmfb`\n",
        "\n",
        "which we produce with the IREE-compiler we previously built\n",
        "\n",
        "```bash\n",
        "iree-compile resnet_50.mlir -o resnet_50.vmfb\\\n",
        "--iree-hal-target-backends=llvm-cpu\\\n",
        "--iree-llvm-link-embedded=false\\\n",
        "--iree-llvm-link-static\\\n",
        "--iree-llvm-static-library-output-path=resnet_50.o\\\n",
        "--iree-vm-target-index-bits=32\\\n",
        "```\n",
        "\n",
        "With IREE's bytecode module we can then generate the embed data\n",
        "\n",
        "```cmake\n",
        "iree_c_embed_data(\n",
        "    NAME\n",
        "      resnet_50_c\n",
        "    IDENTIFIER\n",
        "      iree_static_library_resnet_50\n",
        "    GENERATED_SRCS\n",
        "      resnet_50.vmfb\n",
        "    C_FILE_OUTPUT\n",
        "      resnet_50_c.c\n",
        "    H_FILE_OUTPUT\n",
        "      resnet_50_c.h\n",
        "    FLATTEN\n",
        "    PUBLIC\n",
        ")\n",
        "```\n",
        "\n",
        "For an end-to-end example of this with integration into CMake we point the reader to IREE's static library sample:\n",
        "\n",
        "* [IREE Static Library Sample](https://github.com/openxla/iree/tree/main/samples/static_library)\n",
        "\n",
        "The static library can then be integrated into our simulation, and the simulation's build system as usual."
      ],
      "metadata": {
        "id": "B3npNj3u3Gib"
      }
    }
  ]
}