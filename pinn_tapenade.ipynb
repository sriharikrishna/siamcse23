{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriharikrishna/siamcse23/blob/main/pinn_tapenade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnxrzFesM7P_"
      },
      "source": [
        "# PINN with PyTorch and C/Tapenade\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "We will use the PINN approach to solve the PDE\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial x}=2\\cdot\\frac{\\partial u}{\\partial t}+u,$$\n",
        "\n",
        "with initial condition\n",
        "\n",
        "$$u(x,0)=6\\cdot e^{(-3x)}.$$\n",
        "\n",
        "The PDE has a known analytical solution, which we note here but will not explicitly use in the code:\n",
        "\n",
        "$$u(x,t) = 6e^{(-3x-2t)}$$\n",
        "\n",
        "## Desired Outcome\n",
        "\n",
        "We want to obtain a neural network that models $u$. In other words, the neural network should take $x$ and $t$ as input, and produce an output $u$ that is close to $u(x,t)$. However, we assume that $u(x,t)$ is not explicitly known. Instead, we define the residual function $f$ as\n",
        "\n",
        "\n",
        "$$f = \\frac{\\partial u}{\\partial x} - 2\\cdot\\frac{\\partial u}{\\partial t} - u,$$\n",
        "\n",
        "and train our neural network to minimize $f$ at randomly chosen points within a domain that is defined as $x\\in [0,2]$ and $t \\in [0,1]$ for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "Let us start by importing a few things:\n",
        "\n",
        " - PyTorch, to define our neural network.\n",
        " - Numpy, which will help us interface PyTorch and Ctypes\n",
        " - Ctypes, which we will use to call functions compiled from C"
      ],
      "metadata": {
        "id": "V0se8Bw2DgCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adP5z5DcM7QG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "device = torch.device(\"cpu\")\n",
        "import numpy as np\n",
        "import ctypes\n",
        "from numpy.ctypeslib import ndpointer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network setup\n",
        "\n",
        "Now we create a neural net in PyTorch. We (arbitrarily) choose to use a dense neural network with five layers for this tutorial. You could try to use other layer types or sizes, and/or change the number of layers. As always, this is a tradeoff between fidelity, training time, resource usage, etc.\n",
        "\n",
        "The input of the network is going to consist of two vectors $x$, $t$. These will be interpreted as a vector of pairs $(x_i,t_i)$, and the network will produce an output vector  whose entry $i$ is a prediction for $u(x_i, t_i)$."
      ],
      "metadata": {
        "id": "uOLXAp82EUFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv3cXeO_M7QK"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_layer1 = nn.Linear(2,5)\n",
        "        self.hidden_layer2 = nn.Linear(5,5)\n",
        "        self.hidden_layer3 = nn.Linear(5,5)\n",
        "        self.hidden_layer4 = nn.Linear(5,5)\n",
        "        self.hidden_layer5 = nn.Linear(5,5)\n",
        "        self.output_layer = nn.Linear(5,1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
        "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
        "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
        "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
        "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
        "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
        "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "\n",
        "We will train the neural net using a cost function that has two parts: Part A forces the function to be correct at $t=0$, and Part B enforces the PDE throughout the domain."
      ],
      "metadata": {
        "id": "7m2agYIFufnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A: Initial condition\n",
        "\n",
        "Now we create a number of points (here, arbitrarily, 500) randomly spread along the $x$ coordinate, with $t=0$. For each of these points, we evaluate the initial condition\n",
        "$$u_0 = 6\\cdot e^{(-3*x)}.$$\n",
        "\n",
        "We then define a function that takes a given `net` as an input, evaluates it at our random coordinates, and computes the error between the computed result and the true initial condition using the MSE loss function.\n",
        "\n",
        "Later, we will train the neural network to satisfy the initial condition."
      ],
      "metadata": {
        "id": "t0Jznv8OJ3Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = torch.nn.MSELoss()\n",
        "\n",
        "# Define the input coordinates and evaluate the function in Numpy\n",
        "x_bc = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "t_bc = np.zeros((500,1))\n",
        "u_bc = 6*np.exp(-3*x_bc)\n",
        "# Convert to PyTorch tensors\n",
        "pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
        "pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
        "pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
        "\n",
        "# Evaluate net at the coordinates and compute MSE loss\n",
        "def part_A(net):\n",
        "    net_bc_out = net(pt_x_bc, pt_t_bc)\n",
        "    return mse(net_bc_out, pt_u_bc)"
      ],
      "metadata": {
        "id": "9Po6gAQ9J5l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B: Residual\n",
        "\n",
        "Now we define the residual function $f$. The function computes predictions for\n",
        "$$u,$$ $$u_x :=\\frac{\\partial u}{\\partial x},$$\n",
        "$$u_t :=\\frac{\\partial u}{\\partial t}$$\n",
        "by evaluating the neural network and its gradients computed with autograd, for a given $x$, $t$. With all these ingredients prepared, we compute the residual using the definition of our PDE:\n",
        "$$f = u_x - 2\\cdot u_t - u$$\n",
        "If our neural network was a perfect predictor for $u$, we would expect a zero residual for all inputs. Since the neural network is unlikely to be perfect though, we will get some non-zero result. We will train our neural network to minimize the residual throughout the domain.\n",
        "\n",
        "Note the argument `create_graph=True`, which will allow us to back-propagate through `f` including the calls to `autograd.grad` later, essentially computing second-order derivatives."
      ],
      "metadata": {
        "id": "K8azN52IFN4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, t, net):\n",
        "    u = net(x,t)\n",
        "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
        "    return u_x - 2*u_t - u"
      ],
      "metadata": {
        "id": "VnEWGUXIG2gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined `f`, we can use it to compute part B of the cost function. To this end, we evaluate `f` at $500$ randomly generated coordinates in our domain, and compute the mismatch between the obtained result and the expected result (which is zero).\n",
        "\n",
        "Note that we define `part_B` such that the function `f` can be swapped out for something else, because we will later replace it with a function defined in C and differentiated with Tapenade."
      ],
      "metadata": {
        "id": "qoI7__ZryMep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def part_B(net, residual=f):\n",
        "    x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
        "    all_zeros = np.zeros((500,1))\n",
        "    \n",
        "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
        "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
        "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
        "    \n",
        "    f_out = residual(pt_x_collocation, pt_t_collocation, net) # output of f(x,t)\n",
        "    return mse(f_out, pt_all_zeros)"
      ],
      "metadata": {
        "id": "P4SH1J1MyJRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the PDE\n",
        "\n",
        "Now that we have the loss functions and the general network architecture set up, we can go ahead and train the network.\n",
        "\n",
        "We do this inside a function that creates a new instance and returns it after training, so that we can create new ones and compare them easily later."
      ],
      "metadata": {
        "id": "rroichmYLcM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PcDHNcTM7QW"
      },
      "outputs": [],
      "source": [
        "def train_net(f=f):\n",
        "    net = Net()\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    iterations = 10000\n",
        "    for epoch in range(iterations):\n",
        "        optimizer.zero_grad() # reset gradients\n",
        "        mse_u = part_A(net)   # Loss based on initial conditions\n",
        "        mse_f = part_B(net,f) # Loss based on PDE\n",
        "        loss = mse_u + mse_f  # Combined loss\n",
        "        \n",
        "        loss.backward()  # Back-prop\n",
        "        optimizer.step() # Gradient-based optimization step\n",
        "        \n",
        "        if(epoch%100==0):\n",
        "            with torch.autograd.no_grad():\n",
        "                print(epoch,\"Traning Loss:\",loss.data)\n",
        "    return net\n",
        "\n",
        "net = train_net()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the result\n",
        "\n",
        "Now that the neural net is trained, we can evaluate it at regularly spaced points to create a nice plot.\n",
        "The gory details of using matplotlib are inside a function, so we can re-use it later after training other neural nets."
      ],
      "metadata": {
        "id": "M6zazero2v1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0aqQMD5M7QX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "\n",
        "def plotnet(net):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    x=np.arange(0,2,0.02)\n",
        "    t=np.arange(0,1,0.02)\n",
        "    ms_x, ms_t = np.meshgrid(x, t)\n",
        "    x = np.ravel(ms_x).reshape(-1,1)\n",
        "    t = np.ravel(ms_t).reshape(-1,1)\n",
        "    pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
        "    pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)\n",
        "    pt_u = net(pt_x,pt_t)\n",
        "    u=pt_u.data.cpu().numpy()\n",
        "    ms_u = u.reshape(ms_x.shape)\n",
        "    surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
        "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "plotnet(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a custom function\n",
        "\n",
        "Now we have set up a PINN and trained it. All components of the network and the loss function were defined within PyTorch, and used the built-in backpropagation. What if we want to use our own function and our own automatic differentiation?\n",
        "\n",
        "Let's take it step by step. First, we swap out the PDE evaluation in part B of our cost function for something with custom gradients."
      ],
      "metadata": {
        "id": "oqCbsSGx4Qbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShAI5aCdM7QO"
      },
      "outputs": [],
      "source": [
        "class CustomFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, u_x, u_t, u):\n",
        "        #ctx.save_for_backward(fwdinput)\n",
        "        res = u_x - 2*u_t - u\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        #fwdinput, = ctx.saved_tensors\n",
        "        grad_u = -grad_output\n",
        "        grad_u_t = -2*grad_output\n",
        "        grad_u_x = grad_output\n",
        "        return grad_u_x, grad_u_t, grad_u\n",
        "\n",
        "customfunc = CustomFunc.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You probably saw the commented-out lines about `save_for_backward` and `saved_tensors`. This allows us to store and retrieve intermediate results if the gradients depend on them. Right now we don't need it, because our function is linear (because the PDE is linear).\n",
        "\n",
        "Also, note that the gradients here are manually implemented (no automatic differentiation)! Let's see if we did it correctly by using PyTorch's built-in gradient check routine. Try changing a constant or something else inside the custom backwards pass and see if `gradcheck` fails."
      ],
      "metadata": {
        "id": "7R9LTVv75S32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ_Ugc3aM7QO"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import gradcheck\n",
        "u_x = torch.randn(500,1,dtype=torch.double,requires_grad=True)\n",
        "u_t = torch.randn(500,1,dtype=torch.double,requires_grad=True)\n",
        "u = torch.randn(500,1,dtype=torch.double,requires_grad=True)\n",
        "test = gradcheck(customfunc, (u_x, u_t, u), eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the custom PDE to create a new version of `f` that calls it, called `f_custom`."
      ],
      "metadata": {
        "id": "rs4bALlW55PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_custom(x, t, net):\n",
        "    u = net(x,t)\n",
        "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
        "    return customfunc(u_x, u_t, u)"
      ],
      "metadata": {
        "id": "ceVje7QO6CPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and train the neural net using this new function instead of the original `f`."
      ],
      "metadata": {
        "id": "EgMluecQ7Gb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_custom = train_net(f=f_custom)\n",
        "plotnet(net_custom)"
      ],
      "metadata": {
        "id": "7OKb2ss87L6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom function in C with Tapenade\n",
        "\n",
        "Now for the exciting part. Let's swap out the custom function for something we wrote in C and differentiate it with Tapenade. First, we implement the PDE in C and save it to a file."
      ],
      "metadata": {
        "id": "cEgS0VhA8HKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ctest.c\n",
        "void cfun(float* u_x, float* u_t, float* u, float* res, int size) {\n",
        "    for(int i=0; i<size; i++) {\n",
        "        res[i] = u_x[i] - 2.0f*u_t[i] - u[i];\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "bLwmmm388ekz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've seen before, we need to download and unpack Tapenade."
      ],
      "metadata": {
        "id": "26hAdOUX82hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "if [ ! -f tapenade_3.16/LICENSE.html ]; then\n",
        "wget https://tapenade.gitlabpages.inria.fr/tapenade/distrib/tapenade_3.16.tar\n",
        "tar -xvf tapenade_3.16.tar\n",
        "fi\n",
        "\n",
        "\n",
        "# if the AD runtime library is not yet loaded, download and compile it now.\n",
        "if [ ! -f ADFirstAidKit/adStack.h ]; then\n",
        "    wget tapenade.inria.fr:8080/tapenade/ADFirstAidKit.tar\n",
        "    tar vxf ADFirstAidKit.tar\n",
        "    (cd ADFirstAidKit && gcc -c -O3 -w adStack.c && ar -crs libAD.a *.o)\n",
        "fi"
      ],
      "metadata": {
        "id": "DOqiL8gr8-Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run Tapenade on our code snippet."
      ],
      "metadata": {
        "id": "caxpnF909Gnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/content/tapenade_3.16/bin/tapenade -b ctest.c"
      ],
      "metadata": {
        "id": "8Zuu7Ma49F38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can compile the original code and the derivative."
      ],
      "metadata": {
        "id": "oGBTK9i3-shc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcc -O3 -shared -fPIC ctest.c -o ctest.so\n",
        "gcc -O3 -shared -fPIC -I./ADFirstAidKit -L./ADFirstAidKit ctest_b.c -lAD -o ctest_b.so"
      ],
      "metadata": {
        "id": "E77_gUcC-xS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can load the compiled code so that it can be called from Python. We need to wrap the call to the C functions such that the inputs and outputs are contiguous arrays in host memory. Luckily, Numpy allows us to do this although it is a bit lengthy to read."
      ],
      "metadata": {
        "id": "WjSaNd___bzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxv6O6RzM7QR"
      },
      "outputs": [],
      "source": [
        "lib = ctypes.cdll.LoadLibrary(\"./ctest.so\")\n",
        "fun = lib.cfun\n",
        "lib_b = ctypes.cdll.LoadLibrary(\"./ctest_b.so\")\n",
        "fun_b = lib_b.cfun_b\n",
        "\n",
        "\n",
        "class CustomCFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, u_x, u_t, u):\n",
        "        ctx.save_for_backward(u_x, u_t, u)\n",
        "        u_x_c = np.ascontiguousarray(u_x.detach().cpu().numpy())\n",
        "        u_t_c = np.ascontiguousarray(u_t.detach().cpu().numpy())\n",
        "        u_c = np.ascontiguousarray(u.detach().cpu().numpy())\n",
        "        residual = np.ascontiguousarray(np.zeros(u.size(), dtype=np.float32))\n",
        "\n",
        "        fun.restype = None\n",
        "        fun.argtypes = [ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ctypes.c_size_t]\n",
        "        fun(u_x_c, u_t_c, u_c, residual, u_c.size)\n",
        "        return torch.tensor(residual, requires_grad=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_residual):\n",
        "        u_x, u_t, u = ctx.saved_tensors\n",
        "        u_x_c = np.ascontiguousarray(u_x.detach().cpu().numpy())\n",
        "        u_t_c = np.ascontiguousarray(u_t.detach().cpu().numpy())\n",
        "        u_c = np.ascontiguousarray(u.detach().cpu().numpy())\n",
        "        residual = np.ascontiguousarray(np.zeros(u.size(), dtype=np.float32))\n",
        "        residual_b_c = np.ascontiguousarray(grad_residual.detach().cpu().numpy())\n",
        "        u_x_b_c = np.ascontiguousarray(np.zeros(u.size(), dtype=np.float32))\n",
        "        u_t_b_c = np.ascontiguousarray(np.zeros(u.size(), dtype=np.float32))\n",
        "        u_b_c = np.ascontiguousarray(np.zeros(u.size(), dtype=np.float32))\n",
        "\n",
        "        fun_b.restype = None\n",
        "        fun_b.argtypes = [ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ndpointer(ctypes.c_float, flags=\"C_CONTIGUOUS\"),\n",
        "                        ctypes.c_size_t]\n",
        "        fun_b(u_x_c, u_x_b_c, u_t_c, u_t_b_c, u_c, u_b_c, residual, residual_b_c, u_c.size)\n",
        "        grad_u_x = torch.tensor(u_x_b_c, requires_grad=True)\n",
        "        grad_u_t = torch.tensor(u_t_b_c, requires_grad=True)\n",
        "        grad_u = torch.tensor(u_b_c, requires_grad=True)\n",
        "        return grad_u_x, grad_u_t, grad_u\n",
        "\n",
        "customcfunc = CustomCFunc.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a lot, so let's use gradcheck to see if everything is correct."
      ],
      "metadata": {
        "id": "KfyneO2Y_8ww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx-h93RUM7QT"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import gradcheck\n",
        "u_x = torch.randn(500,1,requires_grad=True)\n",
        "u_t = torch.randn(500,1,requires_grad=True)\n",
        "u = torch.randn(500,1,requires_grad=True)\n",
        "test = gradcheck(customcfunc, (u_x, u_t, u), eps=1e-3, atol=1e-2)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define yet another version of `f`, this time using our custom C function."
      ],
      "metadata": {
        "id": "ZdMU27s2AQ8q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k494-hqBM7QU"
      },
      "outputs": [],
      "source": [
        "def f_custom_C(x, t, net):\n",
        "    u = net(x,t)\n",
        "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
        "    return customcfunc(u_x, u_t, u)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together, let's train again (this time with out custom C function), and plot the results."
      ],
      "metadata": {
        "id": "Qsbo2tcJAgT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvihYxQ3M7QU"
      },
      "outputs": [],
      "source": [
        "net_custom_C = train_net(f=f_custom)\n",
        "plotnet(net_custom_C)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We have trained three equivalent PINN's that were implemented in slightly different ways:\n",
        "\n",
        "1.   A PINN using only built-in PyTorch functions,\n",
        "2.   another one using a custom residual function with manually defined backwards pass, and\n",
        "3.   a final one that used a residual function written in C and differentiated with Tapenade.\n",
        "\n",
        "# Some ideas to try yourself:\n",
        "\n",
        "\n",
        "*   Play with the custom backwards pass and see if `gradcheck` catches any mistakes reliably.\n",
        "*   Try a non-linear PDE, and see how this requires storing and loading intermediate results.\n",
        "*   Implement that non-linear PDE in C and try out Tapenade with it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M-_inrc4A4EO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}