{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Simulations into Machine Learning\n",
        "\n",
        "## About\n",
        "\n",
        "A question we increasingly ask ourselves with modern algorithms is how we can integrate large simulations with machine learning components. Especially when we consider to do this in an intrusive fashion, then we are faced with a number of tough choices, some of which are often counter-intuitive on first glance. This problem is mostly faced when working with a simulation code from a classical HPC language such as C, C++, or Fortran and then seeking to integrate this with a machine learning system in Python such as PyTorch, TensorFlow, or JAX. While some decide to rewrite their simulations in machine learning DSLs, or programming languages with broader automatic differentiation support such as Julia, we focus on the case of keeping the HPC simulation intact, and instead seeking to highlight the potential interfaces between the two. Taking a look at literature we see the downside of potentially overly relying on machine learning DSLs in particular\n",
        "\n",
        "![](https://i.imgur.com/K2V1JbC.png)\n",
        "\n",
        "(Source: [Dr. JIT: A Just-In-Time Compiler for Differentiable Rendering](https://arxiv.org/abs/2202.01284))\n",
        "\n",
        "> Our performance is in large parts governed by our ability to map our operations on the optimized set of computation primitives offered by the machine learning DSL."
      ],
      "metadata": {
        "id": "DgpNNe5r56iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "* [1. Integrating C into Machine Learning](#c-into-ml)\n",
        "  * [1.1 CTypes](#ctypes-ml-c)\n",
        "* [2. Integrating C++ into Machine Learning](#cpp-into-ml)\n",
        "  * [2.1 PyTorch C++ Extension](#pytorch-cpp-extension)\n",
        "    * [2.1.1 PyBind11](#pybind11-cpp)\n",
        "* [3. Integrating Fortran into Machine Learning](#fortran-into-ml)"
      ],
      "metadata": {
        "id": "UY7sYCRR56O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. C Integrating C into Machine Learning <a name=\"c-into-ml\"></a>"
      ],
      "metadata": {
        "id": "XllS4dpQ60eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C offers a variety of pathways to approach this problem, the most prominent of which are [CTypes](https://docs.python.org/3/library/ctypes.html), and [Cython](https://cython.org). For the integration of differentiated code into machine learning frameworks we take the example of PyTorch, but exposing the differentiated code as a library is generally applicable beyond only PyTorch and extends to the TensorFlow/JAX ecosystem.\n",
        "\n",
        "![](https://i.imgur.com/pokz4ge.png)"
      ],
      "metadata": {
        "id": "g04tT2bb60at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> All applicable approaches for C are the same for Tapenade, as well as Enzyme."
      ],
      "metadata": {
        "id": "Dn3hJcbq9-4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 CTypes <a name=\"ctypes-ml-c\"></a>"
      ],
      "metadata": {
        "id": "KTeKlEip7oCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CTypes is a foreign function library for Python, which provides C compatible data types, and hence allows for the calling of functions in DLLs or most importantly shared libraries. By wrapping these libraries we can then call them from our pure Python syntax and combine the two languages with as little friction as possible. Beginning by defining ourselves a test library in C"
      ],
      "metadata": {
        "id": "D5t74_e07n_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c\n",
        "// vjp_enzyme.c\n",
        "extern double __enzyme_fwddiff(void*, double[100], double[100], double[100], double[100]);\n",
        "void f(double x[100], double out[100]) {\n",
        "    int prev = 0;\n",
        "    for(int i = 0; i < 100; i++) {\n",
        "        out[i] = x[i] - prev/x[i];\n",
        "        prev = x[i];\n",
        "    }\n",
        "}\n",
        "void jvpf(double x[100], double v[100], double out[100], double dout[100]) {\n",
        "    __enzyme_fwddiff((void*)f, x, v, out, dout);\n",
        "}\n",
        "```\n",
        "\n",
        "we then have to compile this vjp library into a library object, which we can then expose to Python through CTypes. Presuming an existing Enzyme installation, the next steps then take the following form:\n",
        "\n",
        "```bash\n",
        "clang vjp_enzyme.c -S -emit-llvm -o input.ll -O2 -fno-vectorize -fno-slp-vectorize -fno-unroll-loops\n",
        "opt input.ll -load=/path/to/Enzye/LLVMEnzyme-<LLVM version number>.so -enzyme -o output.ll -S -enable-new-pm=0\n",
        "opt output.ll -O2 -o output_opt.ll -S\n",
        "clang output_opt.ll -lib -o libvjp_enzyme.a\n",
        "```\n",
        "\n",
        "Which we can then call from the Python level with\n",
        "\n",
        "```python\n",
        "import ctypes\n",
        "import numpy as np\n",
        "\n",
        "lib = ctypes.CDLL('libvjp_enzyme.a')\n",
        "\n",
        "# Initializing the values\n",
        "x = np.arange(1, 101, dtype='float64') ** 2\n",
        "y = np.ones(100)\n",
        "\n",
        "# Setting shadow structures manually\n",
        "out = np.zeros(100)\n",
        "dout = np.zeros(100)\n",
        "\n",
        "for a in [x, v, out, dout]:\n",
        "  args.append(a.ctypes.data_as(ctypes.POINTER(ctypes.c_double)))\n",
        "lib.jvpf(*args)\n",
        "\n",
        "# \"Inspect\" the gradient\n",
        "print(dout)\n",
        "```\n",
        "\n",
        "There are multiple avenues CTypes hand-off to PyTorch can be made smooth, and almost seamless. Some of these avenues will be shown in the upcoming example _PINN with PyTorch and Tapenade_."
      ],
      "metadata": {
        "id": "-9eH3cge_sNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We will go more in-depth on the use of CTypes in conjunction with PyTorch in the next hands-on example with Tapenade!"
      ],
      "metadata": {
        "id": "iaAc_GJq-Jm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Seeing the above CTypes examples, we will not spell out the Cython syntax at this point, but point to the [Cython documentation](https://cython.readthedocs.io/en/latest/src/tutorial/cython_tutorial.html) for the exact details to replicate the above with Cython."
      ],
      "metadata": {
        "id": "OwnxIj25L2-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Integrating C++ into Machine Learning <a name=\"cpp-into-ml\"></a>\n",
        "\n",
        "For the integration of differentiated code into machine learning frameworks we take the example of PyTorch, but the integration path of exposing the differentiated code as a library is generally applicable beyond only PyTorch and extends to the TensorFlow/JAX ecosystem.\n",
        "\n",
        "![](https://i.imgur.com/GncIVOw.png)\n",
        "\n",
        "While there exist two great binding packages for C++ in \n",
        "\n",
        "* [PyBind11](https://pybind11.readthedocs.io/en/stable/)\n",
        "* [Nanobind](https://nanobind.readthedocs.io/en/latest/)\n",
        "\n",
        "we first have to define the derivative functions. With the same options from C persisting in CTypes, and Cython, we will not focus further on these options in this section, but instead more deeply look at [PyTorch's C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html) which allows Enzyme to operate within its Just-in-time (JIT) compiler, and compute the gradients as part of the compilation process of individual functions."
      ],
      "metadata": {
        "id": "3WxV5STB-JkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 PyTorch's C++ Extension <a name=\"pytorch-cpp-extension\"></a>"
      ],
      "metadata": {
        "id": "1A57j4nm-Jh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on [PyTorch's C++ example](https://pytorch.org/tutorials/advanced/cpp_extension.html) we will now expand PyTorch's `Function` and `Module` definitions to suit our purposes. Key here is that we are able to include Enzyme, and hence simply attach the gradient information to the existing infrastructure for external functions in PyTorch. Taking the long-long term memory cell example now we would then be looking at the following PyTorch code"
      ],
      "metadata": {
        "id": "OsRUZngdBzIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class LLTM(torch.nn.Module):\n",
        "    def __init__(self, input_features, state_size):\n",
        "        super(LLTM, self).__init__()\n",
        "        self.input_features = input_features\n",
        "        self.state_size = state_size\n",
        "        # 3 * state_size for input gate, output gate and candidate cell gate.\n",
        "        # input_features + state_size because we will multiply with [input, h].\n",
        "        self.weights = torch.nn.Parameter(\n",
        "            torch.empty(3 * state_size, input_features + state_size))\n",
        "        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.state_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, +stdv)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        old_h, old_cell = state\n",
        "        X = torch.cat([old_h, input], dim=1)\n",
        "\n",
        "        # Compute the input, output and candidate cell gates with one MM.\n",
        "        gate_weights = F.linear(X, self.weights, self.bias)\n",
        "        # Split the combined gate weight matrix into its components.\n",
        "        gates = gate_weights.chunk(3, dim=1)\n",
        "\n",
        "        input_gate = torch.sigmoid(gates[0])\n",
        "        output_gate = torch.sigmoid(gates[1])\n",
        "        # Here we use an ELU instead of the usual tanh.\n",
        "        candidate_cell = F.elu(gates[2])\n",
        "\n",
        "        # Compute the new cell state.\n",
        "        new_cell = old_cell + candidate_cell * input_gate\n",
        "        # Compute the new hidden state and output.\n",
        "        new_h = torch.tanh(new_cell) * output_gate\n",
        "\n",
        "        return new_h, new_cell\n",
        "```"
      ],
      "metadata": {
        "id": "G5Ma9NARBzFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Such normal function uses all the optimized kernels PyTorch has implemented in `ATen`, but at the sime time PyTorch enables us to **rewrite parts of our simulations in C++**. We can exploit this interface for our purpose of including differentiated simulations in the machine learning model.\n",
        "\n",
        "\n",
        "###### Ahead-of-Time Compilation\n",
        "\n",
        "The ahead-of-time compilation is then performed through `setuptools.Extension`, which the below code tells to use the `C++` backend of extension.\n",
        "\n",
        "```python\n",
        "rom setuptools import setup, Extension\n",
        "from torch.utils import cpp_extension\n",
        "\n",
        "setup(name='lltm_cpp',\n",
        "      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n",
        "      cmdclass={'build_ext': cpp_extension.BuildExtension})\n",
        "```\n",
        "\n",
        "###### Just-in-Time Compilation\n",
        "\n",
        "For the JIT-compilation we need to have the following lines in setuptools:\n",
        "\n",
        "```python\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "lltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"])\n",
        "```\n",
        "\n",
        "this will lead to the following actions in the backend:\n",
        "\n",
        "* Create a temporary directory to store build artifacts in\n",
        "* Store a `Ninja` build file in the temporary directory\n",
        "* Compile the source files into a shared library\n",
        "* Import the shared library as a Python module"
      ],
      "metadata": {
        "id": "wjpcBw8pBzC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating Enzyme into such a pipeline then requires takes the following shape. On the highest level we have a calling function:\n",
        "\n",
        "```python\n",
        "import lltm\n",
        "\n",
        "a = torch.from_numpy(np.array([[1,2,3,4.]], dtype=np.float32))\n",
        "a.requires_grad_(True)\n",
        "b = lltm.Enzyme(\"test.cpp\", \"f\").apply(a).sum()\n",
        "b.backward()\n",
        "```\n",
        "\n",
        "which under the hood call the same Enzyme automatic differentiation primitive and integrates Enzyme into the JIT pipeline\n",
        "\n",
        "```cpp\n",
        "...\n",
        "std::function<void(void*, void*, size_t, void*)> diffecompile(std::string filename, std::string function) {\n",
        "    int res;\n",
        "\n",
        "    char buffer [L_tmpnam];\n",
        "    tmpnam (buffer);\n",
        "    char data[1024];\n",
        "ENZYME\n",
        "    sprintf(data, \"/usr/bin/clang++-12 -O3 %s -DTF_ENZYME=1 -fno-exceptions -fno-vectorize -fno-slp-vectorize -ffast-math -fno-unroll-loops -Xclang -new-struct-path-tbaa -S -emit-llvm -o %s.ll\", filename.c_str(), buffer);\n",
        "    printf(\"running compile - %s\\n\", data);\n",
        "    res = system(data);\n",
        "    printf(\"ran compile - %s\\n\", data);\n",
        "    assert(res == 0);\n",
        "\n",
        "    sprintf(data, \"/usr/bin/opt-12 %s.ll -load=%s -S -enzyme -mem2reg -instcombine -simplifycfg -adce -loop-deletion -simplifycfg -o %s.ll\", buffer, \"/content/Enzyme-0.0.49/build/Enzyme/LLVMEnzyme-12.so\", buffer);\n",
        "    printf(\"running compile - %s\\n\", data);\n",
        "    res = system(data);\n",
        "    printf(\"ran compile - %s\\n\", data);\n",
        "    assert(res == 0);\n",
        "\n",
        "\n",
        "    printf(\"making buffer 2\\n\");\n",
        "\n",
        "    char buffer2 [L_tmpnam];\n",
        "    printf(\"making tm buffer 2\\n\");\n",
        "    tmpnam (buffer2);\n",
        "    printf(\"made buffer 2\\n\");\n",
        "\n",
        "    sprintf(data, \"/usr/bin/clang++-12 -fPIC -shared %s.ll -o %s.so\", buffer, buffer2);\n",
        "    printf(\"running library - %s\\n\", data);\n",
        "    res = system(data);\n",
        "    printf(\"ran library - %s\\n\", data);\n",
        "    assert(res == 0);\n",
        "\n",
        "    char buffer3[L_tmpnam];\n",
        "    sprintf(buffer3, \"%s.so\", buffer2);\n",
        "\n",
        "    printf(\"running dlopen\\n\");\n",
        "    void* lib = dlopen(buffer3, RTLD_LAZY);\n",
        "    assert(lib);\n",
        "    std::string tofind = \"diffe\" + function;\n",
        "    printf(\"running dlsym %s\\n\", tofind.c_str());\n",
        "    void* sym = dlsym(lib, tofind.c_str());\n",
        "    assert(sym);\n",
        "    auto diffef = (void(*)(void*, void*, size_t, void*))sym;\n",
        "    return diffef;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "po7CAJtsBzAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1 PyBind11 <a name=\"pybind11-cpp\"></a>\n",
        "\n",
        "With PyBind11 and its successor nanobind largely working the same way, we will curtail ourselves here to only focus on the example of PyBind11 which we are applying in conjunction with PyTorch's C++ extension."
      ],
      "metadata": {
        "id": "ViSnVSdC-Jc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of our respective C++ file, we then use PyBind11 to expose those primitives to the Python language level, with which they are then available inside of the machine learning framework.\n",
        "\n",
        "```cpp\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &lltm_forward, \"LLTM forward\");\n",
        "  m.def(\"backward\", &lltm_backward, \"LLTM backward\");\n",
        "}\n",
        "```\n",
        "\n",
        "To recall, what we are providing to PyTorch here are:\n",
        "* Forward function evaluation (no gradients)\n",
        "* Backward function evaluation, i.e. reverse-mode differentiated function\n",
        "\n",
        "> If you want to use PyTorch's newer features such as `vmap` coming out of `torch.fx`, or more advanced automatic differentiation features such as `forward-diff`, then you need to provide more definitions to PyTorch.\n",
        "\n",
        "> If you work in JAX then you have to provide more transformations, as you also have to provide the behaviour under the vmap transform! See [here](https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html) for more information."
      ],
      "metadata": {
        "id": "Fz8sALGj-Jah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Integrating Fortran into Machine Learning <a name=\"fortran-into-ml\"></a>\n",
        "\n",
        "For the integration of differentiated code into machine learning frameworks we take the example of PyTorch, but exposing the differentiated code as a library is generally applicable beyond only PyTorch and extends to the TensorFlow/JAX ecosystem.\n",
        "\n",
        "![](https://i.imgur.com/LNpFhQN.png)"
      ],
      "metadata": {
        "id": "R3QjNFXP-JUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortran, as Fortran has to do for many libraries such as IO, has to rely on C-APIs to interface to machine learning frameworks, as such we refer to the CTypes section above, and in the _PINN with PyTorch and Tapenade_ example."
      ],
      "metadata": {
        "id": "ue1iPT8_7n3i"
      }
    }
  ]
}