{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPocRWWHF6CfqrMc9PJoJCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriharikrishna/siamcse23/blob/main/pytorch_seeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom PyTorch functions and seeding\n",
        "\n",
        "When using PyTorch out of the box, we usually don't need to think about seeding. This is because we normally have a scalar-valued loss function, and calling `backward()` on its result will internally create a \"seed\", although that seed is of size $1$ and is really just one single number: `1.0`\n",
        "Given two successive layers $L_1$ and $L_2$ within the same network, where the output of $L_1$ is used as the input to $L_2$, back-propagation will automatically use the gradient of the loss function with respect to the inputs of $L_2$ as a seed for the gradient computation of $L_1$. All of this happens in the background without users having to know about it.\n",
        "\n",
        "As soon as we are dealing with loss functions that are not scalar-valued (that is, loss functions with multiple output values), or as soon as we are manually implementing the `backward()` pass of an intermediate function or layer within a computation for which we compute gradients, then we need to understand seeding."
      ],
      "metadata": {
        "id": "B06Uo3c9aTVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by importing torch and numpy."
      ],
      "metadata": {
        "id": "0ueUflp4-wUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ezRswj0Z-0Bn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a small neural network. Note how the constructor allows us to swap out one of the layer types, we will use this to insert our custom layer later."
      ],
      "metadata": {
        "id": "-XS0qD7y_irV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, customlayer):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 4, 3)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv2d(4, 5, 3)\n",
        "        self.custom = customlayer\n",
        "        self.max = torch.nn.MaxPool2d(2)\n",
        "        self.linear = torch.nn.Linear(5*6*6, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.custom(x)\n",
        "        x = self.max(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gR6aS6Qt_eBo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to create a simple custom layer that can be used within a PyTorch model. The `forward()` method computes the inference step, and its output is simply the element-wise square of the input. The `backward()` method computes the back-propagation. Given that the inference step computes\n",
        "\n",
        "$$y=x^2,$$\n",
        "\n",
        "we know the derivative to be\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x} = 2\\cdot x.$$\n",
        "\n",
        "This needs to be multiplied with the gradients of the subsequent layer, which is passed to us through `grad_output`. The result of all this will be returned by us as `fwdinput`:\n",
        "\n",
        "$$\\bar{x} = \\frac{\\partial y}{\\partial x}\\cdot\\bar{y} = 2\\cdot x\\cdot\\bar{y}.$$\n",
        "\n",
        "Let's remember that in the above, $\\bar{y}$ represents the gradient of the loss function with respect to $y$, whereas $\\bar{x}$ represents the gradient of the loss function with respect to $x$.\n"
      ],
      "metadata": {
        "id": "iIeR8vVp-5K7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aPwWTKtaaFGs"
      },
      "outputs": [],
      "source": [
        "class CustomLayer(torch.nn.Module):\n",
        "    class __Func__(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, fwdinput):\n",
        "            ctx.save_for_backward(fwdinput)\n",
        "            return fwdinput**2\n",
        "        \n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            fwdinput, = ctx.saved_tensors\n",
        "            grad_input = 2*fwdinput*grad_output\n",
        "            return grad_input\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomLayer, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.__Func__.apply(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check whether we did this correctly, we can use the built-in `gradcheck` routine that ships with Torch."
      ],
      "metadata": {
        "id": "VJjFg9uwGdSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "input = torch.randn(500,1,dtype=torch.double,requires_grad=True)\n",
        "test = gradcheck(CustomLayer.__Func__.apply, input, eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh1gZHPOGmp-",
        "outputId": "71a53061-2ce3-4968-9115-14ff17a883b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to modify the forward or backward pass and see if the gradcheck still works."
      ],
      "metadata": {
        "id": "ItE1mNYJHShT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a neural net using this custom layer, and compute its gradients for some random input point."
      ],
      "metadata": {
        "id": "64k0R1iYAeyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_batches = 2\n",
        "nninput = torch.randn(n_batches, 3, 16, 16)\n",
        "tgt = torch.randn((n_batches, 7))\n",
        "\n",
        "net = Net(CustomLayer())\n",
        "net.zero_grad()\n",
        "out = net(nninput)\n",
        "loss = torch.nn.MSELoss(reduction='sum')\n",
        "l = loss(out, tgt)\n",
        "l.backward()\n",
        "\n",
        "print(f\"grad_conv1_bias:\\n{net.conv1.bias.grad}\")\n",
        "print(f\"grad_conv2_bias:\\n{net.conv2.bias.grad}\")\n",
        "#print(f\"grad_conv1_weight:\\n{net.conv1.weight.grad}\")\n",
        "#print(f\"grad_conv2_weight:\\n{net.conv2.weight.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl8P5ek-AdYr",
        "outputId": "8e74dac1-3d72-4718-8df6-ea435b34d4a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_conv1_bias:\n",
            "tensor([ 0.2400, -1.0586,  0.9492, -2.2431])\n",
            "grad_conv2_bias:\n",
            "tensor([ 0.2569, -0.1768,  2.1923, -1.3565,  3.4123])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to un-comment the print statements for gradients with respect to weight of each layer, but beware that the output is a little lengthy."
      ],
      "metadata": {
        "id": "_svLWYmJBBOq"
      }
    }
  ]
}