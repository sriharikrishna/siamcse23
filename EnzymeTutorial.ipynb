{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6cf45e-6bd8-460b-a4de-ce15bbcbf725",
   "metadata": {},
   "source": [
    "# An Introduction to Enzyme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90299a-7ba4-47a7-890c-2f78c21388bf",
   "metadata": {},
   "source": [
    "In this short tutorial we will introduce the main concepts behind [Enzyme](https://enzyme.mit.edu), a modern compiler plugin within the [LLVM](https://llvm.org) ecosystem which synthesizes gradients while being deeply ingrained in, and leveraging the LLVM compiler infrastructure. The most important core features of Enzyme are in short:\n",
    "\n",
    "* Forward-Mode Derivatives\n",
    "* Reverse-Mode Gradients\n",
    "* Vector-Modes, and Vectorization of Undifferentiated Code\n",
    "* Automatic Differentiation of Parallelism (MPI, MPI+OpenMP, OpenMP, RAJA, CUDA, ROCm)\n",
    "* Cross-Language Automatic Differentiation\n",
    "\n",
    "We will give a glimpse into some of these capabilities with code examples, as well as in-depth looks at key concepts behind Enzyme, and what makes Enzyme different to preceding tools. With Enzyme being a compiler plugin it always requires simulations to be compiled with a LLVM-based compiler into which it can be loaded as a plugin. Such compilers are:\n",
    "\n",
    "* [Clang](https://clang.llvm.org) & Flang(-new)\n",
    "* Intel Compiler Suite: [Intel C/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html#gs.qzto86) & [Intel Fortran Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/fortran-compiler.html#gs.qztmhg)\n",
    "* [AMD Optimizing C/C++ and Fortran Compilers (AOCC)](https://developer.amd.com/tools-and-sdks/)\n",
    "* [ARM HPC Compiler Toolchain](https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/arm-compilers-and-libraries-for-hpc-now-free): ARM C/C++ Compiler & ARM Fortran Compiler\n",
    "\n",
    "While your simulation needs to be compiled with LLVM, you don't require a source installation of any of these compilers to follow this introduction to Enzyme along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eb6e2-3b0e-47bc-8458-8fc997755857",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* [1. Getting Set Up for the Tutorial](#getting-set-up)\n",
    "* [2. The Core Ideas Behind Enzyme's Design](#enzyme-design)\n",
    "* [3. Differentiating First Functions](#first-differentiation)\n",
    "  * [3.1 Square](#_square)\n",
    "  * [3.2 Norm](#_norm)\n",
    "* [4. In-Depth Look at Enzyme's Intermediate Steps in the Case of ReLU-3](#enzyme-relu3)\n",
    "  * [4.1 Activity Analysis](#enzyme-activity-analysis)\n",
    "  * [4.2 Allocation and Zeroing of the Shadow Memory](#enzyme-allocation-shadow)\n",
    "  * [4.3 Computation of the Adjoints](#enzyme-comp-adjoints)\n",
    "  * [4.4 Post Optimization](#enzyme-post-opt)\n",
    "* [5. The Importance of Activity Analysis](#activity-analysis)\n",
    "  * [5.1 Activity Annotations in Code](#enzyme-annotations)\n",
    "* [6. Parallel Automatic Differentiation](#enzyme-parallel-ad)\n",
    "  * [6.1 Seamless Automatic Differentiation of OpenMP](#enzyme-openmp)\n",
    "* [7. Further Resources](#enzyme-further-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f834f-c4c2-4db4-b1db-35c14073da26",
   "metadata": {},
   "source": [
    "## 1. Getting Set Up for the Tutorial <a name=\"getting-set-up\"></a>\n",
    "\n",
    "In order to following with this tutorial there exist two main approaches\n",
    "\n",
    "* Staying inside of this tutorial and utilizing the interactive [compiler explorer instances](https://enzyme.mit.edu/explorer) to interact with the examples\n",
    "* Setting Enzyme up on your device, and run the examples locally\n",
    "\n",
    "The preferred approach we will follow for this SIAM-CSE tutorial will be following the former for the sake of time. But we would highly urge you to sit down and play around with the examples on your local device and compile them for yourself. To do so you'll need to\n",
    "\n",
    "1. Install Enzyme\n",
    "  * [Install Enzyme with Brew](https://formulae.brew.sh/formula/enzyme#default)\n",
    "  * [Install Enzyme with Spack](https://spack.readthedocs.io/en/latest/package_list.html#enzyme)\n",
    "  * [Pull the Enzyme Docker Container](https://github.com/EnzymeAD/enzyme-dev-docker)\n",
    "  * [Install Enzyme from Source](https://enzyme.mit.edu/Installation/)\n",
    "2. [Download the Enzyme-Tutorial](https://github.com/EnzymeAD/Enzyme-Tutorial)\n",
    "\n",
    "We are providing the link to the respective source file examples with each example as we lead through this tutorial. While this is hidden away in the case of the compiler explorer instances, compilation of Enzyme provides a number of shared libraries to invoke Enzyme from different levels of the compilation process:\n",
    "\n",
    "* `ClangEnzyme-<LLVM Version>.so`\n",
    "* `FlangEnzyme-<LLVM Version>.so`\n",
    "* `LLDEnzyme-<LLVM Version>.so`\n",
    "* `LLVMEnzyme-<LLVM Version>.so`\n",
    "\n",
    "The first two are just loaded into the compiler, while `LLDEnzyme` is invoked during the link-time optimization and mostly used for the automatic differentiation of multi-source projects. `LLVMEnzyme` is mostly used for the manual invocation of Enzyme during the optimization process with `opt`, a stage whiuch is usually handled automatically by a compiler such as Clang, or Flang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235268f-f177-477a-bfb6-89140fd36413",
   "metadata": {},
   "source": [
    "## 2. The Core Ideas Behind Enzyme's Design <a name=\"enzyme-design\"></a>\n",
    "\n",
    "In existing automatic differentiation pipelines you customarily have a host language such as C, C++, Fortran, Julia, Rust, etc. in which the automatic differentiation is computed on the level of the host language before the differentiated code is then lowered into the compilation pipeline, where the code is optimized, and the executable generated. Considering languages which utilize the LLVM compiler infrastructure this takes the following shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fcb2d-0503-4ab8-a0a2-e0c354ba61ca",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/uBfAsVh.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7ac12-d68e-4f42-bc57-8da706427fd3",
   "metadata": {},
   "source": [
    "Enzyme is able to leverage being inside of the compiler to run optimizations before gradient synthesization, and benefit from the large library of optimizations written by compiler developers in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2372a7-a062-4213-aff2-b15172a4e3d5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/xtHeCon.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a23f07-78d2-40d6-ae67-62e1878e7e35",
   "metadata": {},
   "source": [
    "### 3. Differentiating First Functions <a name=\"first-differentiation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9784fd5-24b7-4caf-b2ba-06003becc260",
   "metadata": {},
   "source": [
    "#### 3.1 Square <a name=\"_square\"></a>\n",
    "\n",
    "A first simple example we consider to differentiate with Enzyme is a square-function, which takes the derivative with respect to x.\n",
    "\n",
    "```c\n",
    "// Function to differentiate\n",
    "double square(double x) {\n",
    "  return x * x;\n",
    "}\n",
    "```\n",
    "\n",
    "The Enzyme autodiff call is then introduced into the C source-file, along with a number of annotations the purpose of which we will explain in later parts of the tutorial.\n",
    "\n",
    "```c\n",
    "double __enzyme_autodiff(void*, ...);\n",
    "```\n",
    "\n",
    "Initializing a first value to our then differentiated function, we can then obtain the gradient of the square-function at point `x`.\n",
    "\n",
    "```c\n",
    "double grad_x = __enzyme_autodiff((void*)square, x);\n",
    "printf(\"Gradient square(%f) = %f\\n\", x, grad_x);\n",
    "```\n",
    "\n",
    "we then compile this first square-example with clang, or the LLVM-based C-compiler of our choice, loading Enzyme as a LLVM compiler-plugin:\n",
    "\n",
    "```bash\n",
    "clang-12 square.c -O3 -Xclang -load -Xclang /path/to/Enzyme/ClangEnzyme-12.so\n",
    "```\n",
    "\n",
    "An interactive version of this example can be accessed in this saved [Enzyme Explorer instance](https://fwd.gymni.ch/L4Lu2A) or in the Enzyme-Tutorial under `1_square`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7b6c-2c98-434c-b6d4-bfeaba8ad6d7",
   "metadata": {},
   "source": [
    "#### 3.2 Norm <a name=\"_norm\"></a>\n",
    "\n",
    "To consider the asymptotic speed-ups afforded by Enzyme's design into the compiler, we have to consider applying a norm with and without optimizations for which we time the two variants in comparison. The normalization function we consider for this is\n",
    "\n",
    "```c\n",
    "// Normalization function\n",
    "void normalize(double *__restrict__ out, const double *__restrict__ in, int n) {\n",
    " for(int i = 0; i < n; ++i)\n",
    "    out[i] = in[i] / mag(in, n);\n",
    "}\n",
    "```\n",
    "\n",
    "of which we then compile two executables, one of which applies optimizations _before_ the gradient synthesization, and one of which applies optimizations _after_ gradient synthesization. Breaking down the pipelines for the two:\n",
    "\n",
    "1. O2-Optimization -> Enzyme -> O2-Optimization\n",
    "2. Enzyme -> O2-Optimization -> O2-Optimization\n",
    "\n",
    "Which show that while AD on the unoptimized IR takes >400s, the optimization being run before AD results in a runtime of <1s. An interactive version of the unoptimized example can be accessed in this [Enzyme Explorer instance](), the optimized version in this [Enzyme Explorer Instance](), or in the Enzyme-Tutorial under `2_norm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5fa592-7b93-44a0-9a2d-3029fd0c27a2",
   "metadata": {},
   "source": [
    "### 4. In-Depth Look at Enzyme's Intermediate Steps in the Case of ReLU-3 <a name=\"enzyme-relu3\"></a>\n",
    "\n",
    "Looking at a model example of what Enzyme actually does under the hood, we now consider the following ReLU-3 function, consider the intermediate steps involved in the gradient synthesization, and how this translates into LLVM where we are looking at the abstraction of basic blocks of the LLVM IR. The C-function is given by\n",
    "\n",
    "```c\n",
    "double relu3(double x){\n",
    "    double result;\n",
    "    if (x > 0)\n",
    "        result = pow(x, 3);\n",
    "    else\n",
    "        result = 0\n",
    "    return result;\n",
    "}\n",
    "```\n",
    "\n",
    "and invoked as in the first code example above with `__enzyme_autodiff`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a0971-6aca-49fa-ae41-768c26e19a69",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/cQHqoE3.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78413c32-618b-443c-984f-3f75c111311e",
   "metadata": {},
   "source": [
    "#### 4.1 Activity Analysis <a name=\"enzyme-activity-analysis\"></a>\n",
    "\n",
    "Beginning by running a series of analyses, the most important of which is the _Activity Analysis_ which deduces which individual instructions, and which particular values influence the gradient computation. This is done to reduce the amount of total derivative computation by only taking the derivatives which impact the final gradient we are interested in. As this produces a much more compact gradient, it reduces the runtime and makes our derivatives faster. It furthermore avoids attempts at differentiating functions, which are not to be differentiated such as `CPUID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e208914-cd37-4794-bc1c-da27192df872",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/jhaYti6.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a90f1b-c246-475e-8a46-67072f9d0b35",
   "metadata": {},
   "source": [
    "#### 4.2 Allocation and Zeroing of the Shadow Memory <a name=\"enzyme-allocation-shadow\"></a>\n",
    "\n",
    "Next it initializes the shadows, which we are to accumulate the gradients into later. Invoking Enzyme and passing a variable by reference like with a pointer, we then need to _seed_ those shadows. Internal data structures are handled by Enzyme automatically. For the current example we initialize shadows for all of the active variables and set them to 0. Intermediate computation is then accumulated into the shadows throughout the computation. At the end of the computation Enzyme then returns the total derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cb47c-5eaf-41ae-80c1-bd47e0e837df",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/Ilryot1.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdb2cd-9aa6-4e7f-999a-1fc6d0d2d74c",
   "metadata": {},
   "source": [
    "#### 4.3 Computation of the Adjoints <a name=\"enzyme-comp-adjoints\"></a>\n",
    "\n",
    "With this, we can then compute the adjoints where we particularly focus on the $x>0$ branch of the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e648a46-0277-49c7-bc22-de4149536135",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/AyVAzvI.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9ae8c-e207-449c-ba19-50b377088fb7",
   "metadata": {},
   "source": [
    "The derivative of the `pow`/ $x^3$ is then $3 * x^2$ or `3 * pow(x, 2)` and Enzyme performs the corresponding propagation of partial gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5f1ea-e9b6-45d4-a8d5-a092dbbe5b68",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/PmGZzY9.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b294292-b77f-4458-a61c-bcc635d4e082",
   "metadata": {},
   "source": [
    "#### 4.4 Post Optimization <a name=\"enzyme-post-opt\"></a>\n",
    "\n",
    "After the adjoint computation, Enzyme runs further optimization passes to further optimize the code, which if we were to handwrite back into C from the LLVM IR amounts to the optimal gradient we could hand-write ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0db-fdf5-4239-b261-66fb891671c7",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/s13x0oS.png\" width = \"750\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed0716-1747-41b1-9853-a8ff0336dba3",
   "metadata": {},
   "source": [
    "### 5. The Importance of Activity Analysis <a name=\"activity-analysis\"></a>\n",
    "\n",
    "As seen in the previous example, considering and arguing about which variables are relevant to the synthesization of gradients is of utmost importance to performant automatic differentiation. In Enzyme, we have three main activity categories with which we annotate our variables:\n",
    "\n",
    "* `enzyme_const`: Values who don't impact the derivative computation and/or don't need to be differentiated. For example, there is no need to differentiate the size of an array.\n",
    "* `enzyme_dup`: Duplicated arguments are active/differentiable values passed by reference. Examples for this are pointer. `enzyme_dup` furthermore requires you to pass a second, shadow variable to store the derivative information into. Reverse-mode adds the derivative into the shadows, and generally should be zero-initialized as shown in the seeding tutorial. A sub-variant of `enzyme_dup` is `enzyme_dupnoneed`, which only returns the gradient while discarding the original return of the function.\n",
    "* `enzyme_out`: Output arguments which are active/differentiable values passed by value. Examples for this include floats or doubles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd98f9-d7f5-44fe-bb9a-ee552b3e1e8a",
   "metadata": {},
   "source": [
    "#### 5.1 Activity Annotations in Code <a name=\"enzyme-annotations\"></a>\n",
    "\n",
    "To show the difference activity annotations can make to the performance of our differentiated code, we examine the dot-product of two vectors:\n",
    "\n",
    "```c\n",
    "double dot(double* __restrict__ A, double* __restrict__ B, double C, int n) {\n",
    "  double sum = 0;\n",
    "  for (int i=0; i<n; i++) {\n",
    "    sum += A[i] * B[i];\n",
    "  }\n",
    "  return C + sum;\n",
    "}\n",
    "```\n",
    "\n",
    "Providing no activity annotations\n",
    "\n",
    "```c\n",
    "__enzyme_autodiff((void*)dot,\n",
    "                   A, grad_A,\n",
    "                   B, grad_B,\n",
    "                   C,\n",
    "                   n);\n",
    "```\n",
    "\n",
    "and adding activity annotations s.t. Enzyme knows exactly how to handle each provided argument\n",
    "\n",
    "```c\n",
    "__enzyme_autodiff((void*)dot,\n",
    "                   enzyme_dup, A, grad_A,\n",
    "                   enzyme_dup, B, grad_B,\n",
    "                   enzyme_out, C,\n",
    "                   enzyme_const, n);\n",
    "}\n",
    "```\n",
    "\n",
    "Viewing both examples in the compiler explorer, we can see the true impact activity annotations have on the performance of our differentiation:\n",
    "\n",
    "* [No Activity Annotations](https://fwd.gymni.ch/ou4xEJ)\n",
    "* [With Activity Annotations](https://fwd.gymni.ch/caYQGk)\n",
    "\n",
    "With activity annotations giving us a ~33% performance uplift. As you can see from the compilation flags, Enzyme does not require additional flags for the annotations. An example connecting both examples, and showing the performance difference of the two can be found in `3_dot` of the Enzyme-Tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc64f47-9369-4adc-92a1-c2d25e261484",
   "metadata": {},
   "source": [
    "### 6. Parallel Automatic Differentiation <a name=\"enzyme-parallel-ad\"></a>\n",
    "\n",
    "With simulations seeking to become ever more highly resolved, or including ever more physics and hence needing to parallelize ever better the automatic differentiation of such simulations keeps pace. But this introduces multiple difficulties for automatic differentiaton such as the reversal of the control flow transforming a read race in the forward pass into a write race in the reverse pass. Enzyme provides a number of optimizations and strategies to solve these issues for automatic differentiation users, as well as ensure similar scalability to the original program including but not limited to:\n",
    "\n",
    "* Non-atomic load/store\n",
    "* Atomic increments\n",
    "* ...\n",
    "\n",
    "Operating inside of the compiler affords us many niceties in this regard such as the ability to combinatorially combine different parallelization paradigms including\n",
    "\n",
    "* MPI\n",
    "* Julia Threads\n",
    "* OpenMP\n",
    "* CUDA\n",
    "* ROCm\n",
    "\n",
    "Enzyme's approach of operating on the optimized intermediate representation of the compiler is preserved throughout:\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://i.imgur.com/QNgiw4U.png\" width = \"750\">\n",
    "</center>\n",
    "\n",
    "Each implementation of individual parallelization paradigms can be viewed as a basic building block, which can be combined to differentiate higher level parallelization paradigms such as the [RAJA Performance Portability Layer](https://github.com/LLNL/RAJA), without requiring its own custom logic inside of Enzyme. Such combinations of individual automatic differentiation logic are only possible due to Enzyme's deep integration into the compiler and the abstraction level it operates on. Different parallelization paradigms utilizing e.g. fork-join parallelism look the same to Enzyme at the LLVM IR as is better visualized in the following graphic:\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://i.imgur.com/yM1cMUv.png\" width = \"750\">\n",
    "</center>\n",
    "\n",
    "Especially the automatic differentiation of CPU-parallelism is entirely seamless, as is best shown in the following code example with OpenMP parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af5cec-b695-41db-a22a-7cf81fbcbf30",
   "metadata": {},
   "source": [
    "#### 6.1 Seamless Automatic Differentiation of OpenMP <a name=\"enzyme-openmp\"></a>\n",
    "\n",
    "Taking the earlier example of the automatic differentiation of `square`, we now expand this example to utilize an OpenMP `parallel for` region through which Enzyme is able to seamlessly differentiate. Returning to the automatic differentiation of the `square` function, we trivially parallelize the evaluation of the `square`-function for a number of inputs with an OpenMP parallel-for:\n",
    "\n",
    "```c\n",
    "void omp(float *x, int npoints) {\n",
    "#pragma omp parallel for\n",
    "    for (int i = 0; i < npoints; i++) {\n",
    "        x[i] *= x[i];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "To which we can then apply the Enzyme autodiff call in the usual fashion without the necessity of any additional annotation\n",
    "\n",
    "```c\n",
    "double __enzyme_autodiff(void*, ...);\n",
    "\n",
    "__enzyme_autodiff((void*)omp, array, d_array, 1000);\n",
    "```\n",
    "\n",
    "Which is compiled in the usual fashion with the added `-fopenmp` flag customary to all OpenMP-compilation within LLVM-based compilers. An interactive version of this example can be found in the [Enzyme Explorer](https://fwd.gymni.ch/QcHiCN), or in the Enzyme-Tutorial under `openmp/parallel_for`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09d9d2-57f5-44dd-8eeb-e4bd85d127f8",
   "metadata": {},
   "source": [
    "### 7. Further Resources <a name=\"enzyme-further-resources\"></a>\n",
    "\n",
    "\n",
    "While we have now gotten a glimpse into Enzyme's approach to automatic differentiation there are a whole number of features which we have left out:\n",
    "\n",
    "* Forward-Mode Automatic Differentiation\n",
    "* Split-Mode Automatic Differentiation\n",
    "* Vector-Mode Forward & Reverse-Mode\n",
    "* Custom Gradients\n",
    "* Custom Behaviour\n",
    "* Automatic Differentiation of further parallelism such as the ones of MPI, CUDA, and ROCm\n",
    "\n",
    "To explore examples illustrating these individual features, please take a look at:\n",
    "\n",
    "* [Enzyme Tutorial](https://github.com/EnzymeAD/Enzyme-Tutorial)\n",
    "* [Enzyme Documentation: Getting Started](https://enzyme.mit.edu/getting_started/)\n",
    "* [Example of Enzyme in a Makefile with the manual Compilation Steps for Multisource Projects](https://github.com/EnzymeAD/Enzyme-Tutorial/blob/main/9_multisource/Makefile)\n",
    "* [Enzyme CMake Example Project](https://github.com/EnzymeAD/CMake-Template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
